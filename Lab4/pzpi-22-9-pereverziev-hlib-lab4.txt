Міністерство освіти і науки України
Харківський національний університет радіоелектроніки




Кафедра програмної інженерії




Звіт
до лабораторної роботи №4
з «Архітектура програмного забезпечення»












Виконав:                                                                                              Перевірив:
ст. гр. ПЗПІ-22-9                                                                           ас. кафедри ПІ
Переверзєв Г. В                                                                            Дашенков Д.С.
                                                  










Харків 2025
________________
1 ІСТОРІЯ ЗМІН


Таблиця 1 - Історія змін
№
	Дата
	Версія звіту
	Опис змін та виправлень
	1
	08.06.2025
	0.1
	Створено звіт
	



2 ЗАВДАННЯ


Тема: Масштабування бекенду
В цій лабораторній роботі необхідно показати як можна масштабувати бекенд системи для роботи із великим навантаженням. Для цього, можна на вибір: масштабувати сервер горизонтально – багато копій сервера виконують однакові функції для різних користувачів; масштабувати сервер вертикально – різні мікросервіси виконують різні функції і масштабуються окремо одне від одного. На найвищий бал на цю роботу необхідно провести навантажувальне тестування за допомогою Gatling, JMeter, Locust чи іншого подібного інструмента і показати як зі збільшенням кількості серверів зростає кількість запитів на секунду яку витримує система.


________________


3 ОПИС ВИКОНАНОЇ РОБОТИ


3.1 Стратегія масштабування серверної частини
Для забезпечення безперебійної та ефективної роботи серверної частини веб-застосунку OrthoVision реалізовано горизонтальне масштабування за допомогою платформи Kubernetes. Використання ресурсу Deployment дозволяє запускати кілька однотипних копій (реплік) бекенд-сервісу (у поточній конфігурації — дві репліки), що підвищує відмовостійкість системи та збільшує її загальну пропускну здатність.
Кожен інстанс бекенду працює у власному Docker-контейнері, який містить повний набір програмних компонентів і необхідне середовище виконання. Конфігурація підключення до бази даних PostgreSQL здійснюється через змінні оточення, що забезпечує гнучкість, безпеку та зручність налаштувань.
Горизонтальне масштабування полягає у збільшенні кількості ідентичних pod-ів, які одночасно обробляють запити, що дає змогу рівномірно розподілити навантаження між ними. Зміна кількості реплік здійснюється вручну за потреби, наприклад, командою:
Для маршрутизації зовнішніх HTTP-запитів на активні pod-и використовується об’єкт Service типу LoadBalancer, який автоматично розподіляє трафік між репліками та забезпечує доступ до застосунку через зовнішню IP-адресу і порт. Такий підхід гарантує балансування навантаження, високу доступність і стійкість системи до відмов.
Отже, завдяки горизонтальному масштабуванню з використанням Kubernetes, серверна частина OrthoVision досягає необхідної продуктивності, надійності та гнучкості, а також спрощується процес розгортання, оновлення та підтримки застосунку.


  

Рисунок 3.1 – Демонстрація 2 подів-реплік бекенду для реалізації горизонтального масштабування






3.2 Технічна реалізація масштабування
Масштабування бекенду OrthoVision базується на контейнеризації Docker та оркестрації Kubernetes. Серверна частина реалізована на Go, побудована за принципами REST API і не зберігає стан між запитами, що дає змогу запускати кілька ідентичних інстансів сервісу.
Основні технічні рішення:
* Deployment у Kubernetes відповідає за створення і керування подами (екземплярами бекенду). Параметр replicas у YAML-файлі дозволяє гнучко змінювати кількість активних подів без необхідності змінювати код.
* Service типу LoadBalancer забезпечує зовнішній доступ до застосунку. Запити клієнтів балансуються між усіма доступними подами, що рівномірно розподіляє навантаження та підвищує відмовостійкість.
* База даних PostgreSQL розгорнута в окремому поді з підключеним PersistentVolumeClaim, що гарантує збереження даних незалежно від кількості подів бекенду. Всі поди підключаються до єдиної централізованої бази, забезпечуючи консистентність даних.
* Змінні оточення (наприклад, логін, пароль до БД, URL сервісів) передаються в поди через параметр env у deployment.yaml, що дозволяє централізовано управляти конфігурацією.
* Docker-образ бекенду формується через власний Dockerfile, у якому вказана версія Go, усі необхідні залежності та збірка основного файлу main.go.
  

Рисунок 3.2 – Контейнери в Docker Desktop


Посилання на github: https://github.com/NurePereverzievHlib/apz-pzpi-22-9-pereverziev-hlib/tree/main/Lab4


________________


4 ВИСНОВОК
У процесі роботи над проєктом я ознайомився з основами горизонтального масштабування серверної частини застосунку за допомогою Kubernetes. Було розгорнуто кілька екземплярів (pod-ів) бекенду та налаштовано балансування навантаження через LoadBalancer сервіс, що дало змогу зрозуміти, як масштабування впливає на стабільність і потенційну продуктивність системи.
Також я отримав практичні навички в роботі з Kubernetes-ресурсами — такими як Deployment, Service, PVC, — та навчився контролювати життєвий цикл подів, перевіряти логи, діагностувати помилки (CrashLoopBackOff), взаємодіяти з томами, а також оновлювати конфігурації без простоїв.
Окрему увагу було приділено підключенню бази даних у кластері Kubernetes, налаштуванню змінних середовища, і збереженню даних через PersistentVolumeClaim. Це дало глибше розуміння про збереження стану в контейнеризованих середовищах.
Отримані знання створюють хорошу базу для подальшого вивчення тем DevOps, автоматичного масштабування (HPA) та моніторингу продуктивності мікросервісних архітектур.